{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Spacecog Documentation This documentation is supposed to provide a brief introduction into the installation and usage of the Spacecog model. Original author: Micha Burkhardt, michaburkhardt@outlook.com Feel free to contact me if you encounter any issues.","title":"Home"},{"location":"index.html#spacecog-documentation","text":"This documentation is supposed to provide a brief introduction into the installation and usage of the Spacecog model. Original author: Micha Burkhardt, michaburkhardt@outlook.com Feel free to contact me if you encounter any issues.","title":"Spacecog Documentation"},{"location":"files.html","text":"File Structure This section will give you a brief introduction to all the individual scripts. General structure combined/ : main files for the integrated model Results/ -> folder for simulation results combined.py -> main simulation file, integrates all model parts and runs the simulation combinedHelper.py -> helper functions for visualization, video generation, and evaluation combinedNet.py -> connects the VIS and LIP models (when generating connections) combinedNet_loadConn.py -> connects the VIS and LIP models (when loading conections) combinedVis_LIP.py -> main functions for the integrated use of the LIP and VIS models config.yaml -> config file for the network SM/ : Spatial memory and imagery model (Bicanski & Burgess, 2018) data_from_training/ -> pre-generated weights for the SM model targets / -> target stimuli for plotting SM_BoundaryCueing.py -> cue with spatial boundaries SM_PdrivePW_withObj.py -> spatial updating with objects SM_helper_functionsp.y -> helper functions for cueing and coordinate systems SM_init.py -> inits the network SM_integrated -> integrated SM functions for the main simulation SM_load_mat_files.py -> loads weights SM_network.py -> connections between SM populations SM_neuron_model -> neuron models for the SM model SM_parameters.py -> parameters SM_roomGridPositionS.py -> get live positional data for objects (only for 3 targets) SM_updateWTS2.py -> update weights after encoding an object SM_visualize.py -> visualization and plotting SM_vr_functions.py -> interactions with Unity LIP/ : LIP model (Bergelt & Hamker, 2019) LIP_connectionPattern.py -> creates custom connection patterns between populations LIP_generateSignals.py -> generates eye position and attention signal LIP_network.py -> neuron models and connections between populations (generate) LIP_network_loadConn.py -> neuron models and connections between populations (load) LIP_parameters.py -> parameters VIS/ : Visual model (Beuth, 2019) Data/ -> pre-trained weights for the three target objects VIS_Connections.py -> custom connections between VIS populations VIS_InitializationVR.py -> initializes parameters for V1 preprocessing VIS_MainHelperVR.py -> helper functions for VIS model VIS_Network.py -> neuron models and connections between population VIS_ParamVR.py -> Unity related parameters VIS_PreprocessingVR.py -> prepares the pre-processed image VIS_SaccGenVR.py -> saccade generator VIS_SaveResultsVR.py ->savig and plotting VIS_VisualVR.py -> visualization functions VIS_funcitons.py -> additional helper functions VIS_parameters.py -> main parameters for the network UnityVR/ : Unity environment and network interface newAgentBrain -> python side of network interface built with protcol buffers unity2019 -> Unity project to be run on a separate windows machine Python combined.py This is the main file of the model. After importing all necessary modules including ANNarchy and the Network Interface, the network is compiled. It then executes the individual steps of the simulation numbered from 1-6: walk to target object get position of target object encode target object walk to different position recall target object execute saccade Weights The model contains two sets of weights. SM model VIS model SM Model The weights of the SM model can be calculated with the MATLAB script from the eLife publication: https://github.com/bicanski/HBPcollab/tree/master/SpatialEpisodicMemoryModel-v1.0 With some work, some other geometries should be possible as well. VIS The VIS model needs pre-learned weights for object localization which were generated with the one-shot learning procedure (Beuth, 2019). They are stored in VIS/Data/3obj_5rot_2size.mat and contain Three objects: Green and yellow crane, green racecar 5 rotations and 2 sizes for each object This selection is a pretty good fit for not only computational speed but also for robustness, as the VIS model gets very nervous with the small sizes of the objects being included into the weights. If different sizes and rotations are needed, you can create them from weightsObjLoc.mat . With a shape of 180x1505, the ordering along the first dimension is: 1-60: yellow crane 61-120: green crane 121-180: green racecar Within each object: 12 rotations (counter-clockwise, starting with the object facing the camera) and five sizes, starting with the largest size Unity This section will give you a brief introduction to all the individual Unity scripts. unity2019/ This folder contains the complete Unity project including networking scripts. The most important parts are the Assets/ folder and then APPConfig.config APPConfig.config This file includes some config parameters. The most important ones are: <LocalPort>1337</LocalPort> // Unity port <IPAddress>192.168.0.12</IPAddress> // Local IP <sphericalProjection>true</sphericalProjection> // use the spherical projection Assets/ All Unity assets, scripts and the shperical projection shader are in this folder. Useful scripts might be (and contain among other things): AgentScript.cs -> Mostly eye and body movements of the agent BehaviourScript.cs -> Main control class for the environment, change your IP here SpatialCognitionAgentScript.cs -> Agent movement on specific path SpatialCognitionBehaviourScript.cs -> Shader, A* search, initial agent position","title":"File Structure"},{"location":"files.html#file-structure","text":"This section will give you a brief introduction to all the individual scripts.","title":"File Structure"},{"location":"files.html#general-structure","text":"combined/ : main files for the integrated model Results/ -> folder for simulation results combined.py -> main simulation file, integrates all model parts and runs the simulation combinedHelper.py -> helper functions for visualization, video generation, and evaluation combinedNet.py -> connects the VIS and LIP models (when generating connections) combinedNet_loadConn.py -> connects the VIS and LIP models (when loading conections) combinedVis_LIP.py -> main functions for the integrated use of the LIP and VIS models config.yaml -> config file for the network SM/ : Spatial memory and imagery model (Bicanski & Burgess, 2018) data_from_training/ -> pre-generated weights for the SM model targets / -> target stimuli for plotting SM_BoundaryCueing.py -> cue with spatial boundaries SM_PdrivePW_withObj.py -> spatial updating with objects SM_helper_functionsp.y -> helper functions for cueing and coordinate systems SM_init.py -> inits the network SM_integrated -> integrated SM functions for the main simulation SM_load_mat_files.py -> loads weights SM_network.py -> connections between SM populations SM_neuron_model -> neuron models for the SM model SM_parameters.py -> parameters SM_roomGridPositionS.py -> get live positional data for objects (only for 3 targets) SM_updateWTS2.py -> update weights after encoding an object SM_visualize.py -> visualization and plotting SM_vr_functions.py -> interactions with Unity LIP/ : LIP model (Bergelt & Hamker, 2019) LIP_connectionPattern.py -> creates custom connection patterns between populations LIP_generateSignals.py -> generates eye position and attention signal LIP_network.py -> neuron models and connections between populations (generate) LIP_network_loadConn.py -> neuron models and connections between populations (load) LIP_parameters.py -> parameters VIS/ : Visual model (Beuth, 2019) Data/ -> pre-trained weights for the three target objects VIS_Connections.py -> custom connections between VIS populations VIS_InitializationVR.py -> initializes parameters for V1 preprocessing VIS_MainHelperVR.py -> helper functions for VIS model VIS_Network.py -> neuron models and connections between population VIS_ParamVR.py -> Unity related parameters VIS_PreprocessingVR.py -> prepares the pre-processed image VIS_SaccGenVR.py -> saccade generator VIS_SaveResultsVR.py ->savig and plotting VIS_VisualVR.py -> visualization functions VIS_funcitons.py -> additional helper functions VIS_parameters.py -> main parameters for the network UnityVR/ : Unity environment and network interface newAgentBrain -> python side of network interface built with protcol buffers unity2019 -> Unity project to be run on a separate windows machine","title":"General structure"},{"location":"files.html#python","text":"","title":"Python"},{"location":"files.html#combinedpy","text":"This is the main file of the model. After importing all necessary modules including ANNarchy and the Network Interface, the network is compiled. It then executes the individual steps of the simulation numbered from 1-6: walk to target object get position of target object encode target object walk to different position recall target object execute saccade","title":"combined.py"},{"location":"files.html#weights","text":"The model contains two sets of weights. SM model VIS model SM Model The weights of the SM model can be calculated with the MATLAB script from the eLife publication: https://github.com/bicanski/HBPcollab/tree/master/SpatialEpisodicMemoryModel-v1.0 With some work, some other geometries should be possible as well. VIS The VIS model needs pre-learned weights for object localization which were generated with the one-shot learning procedure (Beuth, 2019). They are stored in VIS/Data/3obj_5rot_2size.mat and contain Three objects: Green and yellow crane, green racecar 5 rotations and 2 sizes for each object This selection is a pretty good fit for not only computational speed but also for robustness, as the VIS model gets very nervous with the small sizes of the objects being included into the weights. If different sizes and rotations are needed, you can create them from weightsObjLoc.mat . With a shape of 180x1505, the ordering along the first dimension is: 1-60: yellow crane 61-120: green crane 121-180: green racecar Within each object: 12 rotations (counter-clockwise, starting with the object facing the camera) and five sizes, starting with the largest size","title":"Weights"},{"location":"files.html#unity","text":"This section will give you a brief introduction to all the individual Unity scripts.","title":"Unity"},{"location":"files.html#unity2019","text":"This folder contains the complete Unity project including networking scripts. The most important parts are the Assets/ folder and then APPConfig.config","title":"unity2019/"},{"location":"files.html#appconfigconfig","text":"This file includes some config parameters. The most important ones are: <LocalPort>1337</LocalPort> // Unity port <IPAddress>192.168.0.12</IPAddress> // Local IP <sphericalProjection>true</sphericalProjection> // use the spherical projection","title":"APPConfig.config"},{"location":"files.html#assets","text":"All Unity assets, scripts and the shperical projection shader are in this folder. Useful scripts might be (and contain among other things): AgentScript.cs -> Mostly eye and body movements of the agent BehaviourScript.cs -> Main control class for the environment, change your IP here SpatialCognitionAgentScript.cs -> Agent movement on specific path SpatialCognitionBehaviourScript.cs -> Shader, A* search, initial agent position","title":"Assets/"},{"location":"setup.html","text":"Initial Setup This page will provide some information about how to set up the neural network and Unity on different plattforms. Generally, the computational network is meant to be used in combination with a Unity environment, which provides sensory information from the agent. However, simulation data from the paper is provided to run some simulations without Unity (see next page). So if you just want to get the network up and running, you can skip the Unity installation. Computational Model The computational model is build with Python3 and the ANNarchy neural simulator. To run and install ANNarchy you need a GNU/Linux or OSX operating system. If you have a Windows PC, you need to use the Windows subsystem for linux (WSL2). Linux with Python from distribution Navigate into the Burkhardt2023_Spacecog folder Get all the required python packages (this might vary if you manage your python packages in a different way): pip3 install -r requirements.txt --user Linux with Anaconda/Miniconda It might be smart to create a new conda environment: conda create --name spacecog python=3.9.2 conda activate spacecog Navigate into the Burkhardt2023_Spacecog folder Get all the required python packages (this might vary if you manage your python packages in a different way): pip install -r requirements.txt Windows (with WSL2) Install e.g. Ubuntu in a WSL2 environment. All further steps will have to be performed in WSL2. https://www.omgubuntu.co.uk/how-to-install-wsl2-on-windows-10 Follow one of the two steps above (Linux with Python from distribution / Linux with Anaconda/Miniconda) Virtual Environment The model uses Unity 2019.2.17f1, which has to be installed on a Windows PC. In Unity, the cognitive agent Felice will move through her room and provide the neural network with visual (and high-level positional) information. Install Unity 2019.2.17f1: https://unity3d.com/de/get-unity/download/archive/ Get the unity2019 folder from the repository.","title":"Initial Setup"},{"location":"setup.html#initial-setup","text":"This page will provide some information about how to set up the neural network and Unity on different plattforms. Generally, the computational network is meant to be used in combination with a Unity environment, which provides sensory information from the agent. However, simulation data from the paper is provided to run some simulations without Unity (see next page). So if you just want to get the network up and running, you can skip the Unity installation.","title":"Initial Setup"},{"location":"setup.html#computational-model","text":"The computational model is build with Python3 and the ANNarchy neural simulator. To run and install ANNarchy you need a GNU/Linux or OSX operating system. If you have a Windows PC, you need to use the Windows subsystem for linux (WSL2).","title":"Computational Model"},{"location":"setup.html#linux-with-python-from-distribution","text":"Navigate into the Burkhardt2023_Spacecog folder Get all the required python packages (this might vary if you manage your python packages in a different way): pip3 install -r requirements.txt --user","title":"Linux with Python from distribution"},{"location":"setup.html#linux-with-anacondaminiconda","text":"It might be smart to create a new conda environment: conda create --name spacecog python=3.9.2 conda activate spacecog Navigate into the Burkhardt2023_Spacecog folder Get all the required python packages (this might vary if you manage your python packages in a different way): pip install -r requirements.txt","title":"Linux with Anaconda/Miniconda"},{"location":"setup.html#windows-with-wsl2","text":"Install e.g. Ubuntu in a WSL2 environment. All further steps will have to be performed in WSL2. https://www.omgubuntu.co.uk/how-to-install-wsl2-on-windows-10 Follow one of the two steps above (Linux with Python from distribution / Linux with Anaconda/Miniconda)","title":"Windows (with WSL2)"},{"location":"setup.html#virtual-environment","text":"The model uses Unity 2019.2.17f1, which has to be installed on a Windows PC. In Unity, the cognitive agent Felice will move through her room and provide the neural network with visual (and high-level positional) information. Install Unity 2019.2.17f1: https://unity3d.com/de/get-unity/download/archive/ Get the unity2019 folder from the repository.","title":"Virtual Environment"},{"location":"usage.html","text":"Quick start Computational Model You can perform the initial setup and some testing simulations without using the virtual environment. Navigate to the combined/ folder and run the combined.py script. Using multiple jobs is advised. Depending on how many cores you have available on your system, you could use e.g. 8 jobs. Additionally, at least 32GB of RAM are recommended, but on a native Linux system you might get away with 16GB as long as you don't record and save any neuronal activity. With python from distribution run: python3 combined.py -j8 # run on 8 cores With Anaconda/Miniconda run: python combined.py -j8 # run on 8 cores This should generate a few large connections and will afterwards compile the network. Once compiled, the connections will be saved and a default simulation will be run (the whole process will take roughly 10-20 minutes, depending on your system) After the simulation is finished, navigate to the Results/ folder to see some results from the simulation. This simulation is a testing version, which only includes basic object localization on a pre-saved image and you are very limited with what you can do here. However, if you want, you can change the searched object in the config.yaml file. SET_TARGET: True TARGET: 1 # chose between 0 (yellow crane), 1 (green crane), 2 (green racecar) You can now also use the previously saved connections in your future simulations to speed up the startup process by setting create_connections: False For most other things, you will need to set up the virtual environment as described in the next section. Virtual Environment We start with inizializing the virtual environment: Navigate to unity2019->APPConfig.config and change the IP to your local IP. <IPAddress>YOURIP</IPAddress> Open the Unity Hub and open the unity2019 folder as a project. This might take several minutes, as Unity has to build the required modules. You will get many errors when starting for the first time after compiling. Restart Unity, and they should be gone. In the bottom menu, navigate to Project -> Assets/Scenes and open the SpatialCognition.unity scene. Start the scene by pressing the play button at the top. If everything is good to go, Felice should make a pointing animation towards the target objects. To test the network interface, you can now run the demo.py script on your server, which will run the computational network. The file is located in the UnityVR/newAgentbrain/ folder, and you will need to add the IP of the Unity computer. To use Unity with the computational model, we need one last setup step on the Python side: Open the config.yaml file which is located in the model folder and insert the IP address of your Unity machine: VR_IP_ADDRESS = 'YOURIP' You can now activate the VR in the config.yaml RemoteDesktop: True All other options in the config.yaml can now be used in combination with Unity. If you get a timeout when running the model, you might need to create a rule in the Windows firewall, which allows connections between Unity and your compute server.","title":"Quickstart"},{"location":"usage.html#quick-start","text":"","title":"Quick start"},{"location":"usage.html#computational-model","text":"You can perform the initial setup and some testing simulations without using the virtual environment. Navigate to the combined/ folder and run the combined.py script. Using multiple jobs is advised. Depending on how many cores you have available on your system, you could use e.g. 8 jobs. Additionally, at least 32GB of RAM are recommended, but on a native Linux system you might get away with 16GB as long as you don't record and save any neuronal activity. With python from distribution run: python3 combined.py -j8 # run on 8 cores With Anaconda/Miniconda run: python combined.py -j8 # run on 8 cores This should generate a few large connections and will afterwards compile the network. Once compiled, the connections will be saved and a default simulation will be run (the whole process will take roughly 10-20 minutes, depending on your system) After the simulation is finished, navigate to the Results/ folder to see some results from the simulation. This simulation is a testing version, which only includes basic object localization on a pre-saved image and you are very limited with what you can do here. However, if you want, you can change the searched object in the config.yaml file. SET_TARGET: True TARGET: 1 # chose between 0 (yellow crane), 1 (green crane), 2 (green racecar) You can now also use the previously saved connections in your future simulations to speed up the startup process by setting create_connections: False For most other things, you will need to set up the virtual environment as described in the next section.","title":"Computational Model"},{"location":"usage.html#virtual-environment","text":"We start with inizializing the virtual environment: Navigate to unity2019->APPConfig.config and change the IP to your local IP. <IPAddress>YOURIP</IPAddress> Open the Unity Hub and open the unity2019 folder as a project. This might take several minutes, as Unity has to build the required modules. You will get many errors when starting for the first time after compiling. Restart Unity, and they should be gone. In the bottom menu, navigate to Project -> Assets/Scenes and open the SpatialCognition.unity scene. Start the scene by pressing the play button at the top. If everything is good to go, Felice should make a pointing animation towards the target objects. To test the network interface, you can now run the demo.py script on your server, which will run the computational network. The file is located in the UnityVR/newAgentbrain/ folder, and you will need to add the IP of the Unity computer. To use Unity with the computational model, we need one last setup step on the Python side: Open the config.yaml file which is located in the model folder and insert the IP address of your Unity machine: VR_IP_ADDRESS = 'YOURIP' You can now activate the VR in the config.yaml RemoteDesktop: True All other options in the config.yaml can now be used in combination with Unity. If you get a timeout when running the model, you might need to create a rule in the Windows firewall, which allows connections between Unity and your compute server.","title":"Virtual Environment"}]}